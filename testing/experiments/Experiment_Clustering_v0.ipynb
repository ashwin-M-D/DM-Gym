{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# K-Clustering using Reinforcement Learning\n",
    "\n",
    "## DM-Gym prototype testing\n",
    "\n",
    "### By Ashwin Devanga"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import Base Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### import datamining gym packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dm_gym.utils.data_gen import data_gen_clustering\r\n",
    "from dm_gym.create_env import ray_create_env"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### import ray packages for prebuilt RL models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\r\n",
    "import ray\r\n",
    "from ray.rllib import agents\r\n",
    "from ray import tune"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Function to register environment with ray[tune]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def register_env(env_name, env_config={}):\r\n",
    "    env = ray_create_env(env_name)\r\n",
    "    tune.register_env(env_name, \r\n",
    "        lambda env_name: env(env_name,\r\n",
    "            env_config=env_config))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sample Data Generation (Simulated data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 2 ###Number of dimentions in the data\r\n",
    "k = 3 ###Number of clusters we want in the data\r\n",
    "\r\n",
    "num_records = 100\r\n",
    "parameter_means = []\r\n",
    "parameter_sd = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_gen = data_gen_clustering()\r\n",
    "\r\n",
    "error, error_code, pm, psd = data_gen.param_init(n=n, k=k, num_records=num_records,\r\n",
    "                                                 parameter_means=parameter_means, parameter_sd=parameter_sd)\r\n",
    "data = data_gen.gen_data()\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Plot the data\r\n",
    "plt.scatter(data[1], data[2])\r\n",
    "plt.savefig(\"data_plotted.svg\", dpi=300)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run Mean-Shift model on the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df, coords = data_gen.gen_model(data)\r\n",
    "for cls in final_df['Class'].unique():\r\n",
    "    plt.scatter(final_df[final_df['Class'] == cls][1], final_df[final_df['Class'] == cls][2])\r\n",
    "plt.savefig(\"expected_output.svg\", dpi=300)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model and environment configurations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_name = \"clustering-v0\"\r\n",
    "\r\n",
    "env_config = {\r\n",
    "    'data': data,\r\n",
    "    'k': k,\r\n",
    "}\r\n",
    "\r\n",
    "epochs = 1000\r\n",
    "\r\n",
    "rl_config = dict(\r\n",
    "    log_level = \"ERROR\",\r\n",
    "    env=env_name,\r\n",
    "    \r\n",
    "    num_workers=10,\r\n",
    "    num_gpus=1,\r\n",
    "    \r\n",
    "    env_config=env_config,\r\n",
    "    \r\n",
    "    double_q=True,\r\n",
    "    model=dict(\r\n",
    "        vf_share_layers=False,\r\n",
    "        fcnet_activation='relu',\r\n",
    "        fcnet_hiddens=[128, 64]\r\n",
    "    ),\r\n",
    "    exploration_config={\r\n",
    "        \"type\": \"EpsilonGreedy\",\r\n",
    "        \"initial_epsilon\": 1.0,\r\n",
    "        \"final_epsilon\": 0.02,\r\n",
    "        \"epsilon_timesteps\": 0.9*epochs,\r\n",
    "    },\r\n",
    "    evaluation_config={\r\n",
    "        \"explore\": False,\r\n",
    "    },\r\n",
    "    gamma = 1,\r\n",
    "    target_network_update_freq=500,\r\n",
    "    buffer_size=100,\r\n",
    "    adam_epsilon=1e-8,\r\n",
    "    grad_clip=40,\r\n",
    "    train_batch_size=32,\r\n",
    "    framework='torch',\r\n",
    "    lr=1e-5\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Register environment\r\n",
    "ray.shutdown()\r\n",
    "register_env(env_name, env_config)\r\n",
    "\r\n",
    "# Initialize Ray and Build Agent\r\n",
    "info = ray.init(num_cpus=10, ignore_reinit_error=True, log_to_driver=False)\r\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\r\n",
    "\r\n",
    "agent = agents.dqn.DQNTrainer(env=env_name,\r\n",
    "    config=rl_config)\r\n",
    "results = []\r\n",
    "\r\n",
    "rew = np.nan\r\n",
    "\r\n",
    "pbar = tqdm(range(epochs), desc='Training Loop' )\r\n",
    "\r\n",
    "for i in pbar:\r\n",
    "    res = agent.train()\r\n",
    "    results.append(res)\r\n",
    "    rew = res['episode_reward_mean']\r\n",
    "    pbar.set_description(\"reward = %f\" % rew)\r\n",
    "ray.shutdown()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot Rewards"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib import gridspec\r\n",
    "# Unpack values from each iteration\r\n",
    "rewards = np.hstack([i['episode_reward_mean'] \r\n",
    "    for i in results])\r\n",
    "\r\n",
    "p = 50\r\n",
    "\r\n",
    "mean_rewards = np.array([np.mean(rewards[i-p:i+1]) \r\n",
    "                if i >= p else np.mean(rewards[:i+1]) \r\n",
    "                for i, _ in enumerate(rewards)])\r\n",
    "std_rewards = np.array([np.std(rewards[i-p:i+1])\r\n",
    "               if i >= p else np.std(rewards[:i+1])\r\n",
    "               for i, _ in enumerate(rewards)])\r\n",
    "\r\n",
    "#plt.figure(constrained_layout=True, figsize=(16, 9))\r\n",
    "plt.fill_between(np.arange(len(mean_rewards)), \r\n",
    "                 mean_rewards - std_rewards, \r\n",
    "                 mean_rewards + std_rewards, \r\n",
    "                 label='Standard Deviation', alpha=0.3)\r\n",
    "plt.plot(mean_rewards, label='Mean Rewards')\r\n",
    "plt.ylabel('Rewards')\r\n",
    "plt.xlabel('Episode')\r\n",
    "plt.title('Training Rewards')\r\n",
    "plt.legend()\r\n",
    "plt.savefig(\"Results_Rewards.svg\", dpi=300)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot Loss (td error)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "td_err = [\r\n",
    "    i['info']['learner']['default_policy']['mean_td_error'] \r\n",
    "    for i in results]\r\n",
    "\r\n",
    "\r\n",
    "p = 50\r\n",
    "\r\n",
    "mean_td_err = np.array([np.mean(td_err[i-p:i+1]) \r\n",
    "                if i >= p else np.mean(td_err[:i+1]) \r\n",
    "                for i, _ in enumerate(td_err)])\r\n",
    "std_td_err = np.array([np.std(td_err[i-p:i+1])\r\n",
    "               if i >= p else np.std(td_err[:i+1])\r\n",
    "               for i, _ in enumerate(td_err)])\r\n",
    "\r\n",
    "#plt.figure(constrained_layout=True, figsize=(16, 9))\r\n",
    "plt.fill_between(np.arange(len(mean_td_err)), \r\n",
    "                 mean_td_err - std_td_err, \r\n",
    "                 mean_td_err + std_td_err, \r\n",
    "                 label='Standard Deviation', alpha=0.3)\r\n",
    "plt.plot(mean_td_err, label='Mean td_err')\r\n",
    "plt.ylabel('td_err')\r\n",
    "plt.xlabel('Episode')\r\n",
    "plt.title('Training td_err')\r\n",
    "plt.legend()\r\n",
    "plt.savefig(\"Results_TD_err.svg\", dpi=300)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run through the environment with the agent to get final output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dm_gym.create_env import create_env\r\n",
    "\r\n",
    "env = create_env(env_name, env_config=env_config)\r\n",
    "\r\n",
    "episode_reward = 0\r\n",
    "done = False\r\n",
    "obs = env.reset()\r\n",
    "while not done:\r\n",
    "    action = agent.compute_action(obs)\r\n",
    "    obs, reward, done, info = env.step(action)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Plot final output\r\n",
    "final_df = info['final_state_data']\r\n",
    "for cls in final_df['action'].unique():\r\n",
    "    plt.scatter(final_df[final_df['action'] == cls][1], final_df[final_df['action'] == cls][2])\r\n",
    "plt.savefig(\"rl_predicted_output.svg\", dpi=300)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "interpreter": {
   "hash": "dab403dcfa4a64bee3ff417c650bc5376500f360e3ead239cab01b685475af7b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}