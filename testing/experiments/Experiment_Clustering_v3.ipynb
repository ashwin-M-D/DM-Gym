{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Clustering using Reinforcement Learning\n",
    "\n",
    "## DM-Gym prototype testing\n",
    "\n",
    "### By Ashwin Devanga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Base Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import datamining gym packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_gym.utils.data_gen import data_gen_clustering\n",
    "from dm_gym.create_env import ray_create_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import ray packages for prebuilt RL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to register environment with ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(env_name, env_config={}):\n",
    "    env = ray_create_env(env_name)\n",
    "    tune.register_env(env_name, \n",
    "        lambda env_name: env(env_name,\n",
    "            env_config=env_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data Generation (Simulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 ###Number of dimentions in the data\n",
    "k = 3 ###Number of clusters we want in the data\n",
    "\n",
    "num_records = 100\n",
    "parameter_means = []\n",
    "parameter_sd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data_gen_clustering()\n",
    "\n",
    "error, error_code, pm, psd = data_gen.param_init(n=n, k=k, num_records=num_records,\n",
    "                                                 parameter_means=parameter_means, parameter_sd=parameter_sd)\n",
    "data = data_gen.gen_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the data\n",
    "plt.scatter(data[1], data[2])\n",
    "plt.savefig(\"data_plotted.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Mean-Shift model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df, coords = data_gen.gen_model(data)\n",
    "for cls in final_df['Class'].unique():\n",
    "    plt.scatter(final_df[final_df['Class'] == cls][1], final_df[final_df['Class'] == cls][2])\n",
    "plt.savefig(\"expected_output.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and environment configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"clustering-v3\"\n",
    "\n",
    "env_config = {\n",
    "    'data': data,\n",
    "    'k': k,\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "rl_config = dict(\n",
    "    log_level = \"ERROR\",\n",
    "    env=env_name,\n",
    "    \n",
    "    num_workers=10,\n",
    "    num_gpus=1,\n",
    "    \n",
    "    env_config=env_config,\n",
    "    \n",
    "    double_q=True,\n",
    "    model=dict(\n",
    "        vf_share_layers=False,\n",
    "        fcnet_activation='relu',\n",
    "        fcnet_hiddens=[128, 64]\n",
    "    ),\n",
    "    exploration_config={\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.02,\n",
    "        \"epsilon_timesteps\": 0.9*epochs,\n",
    "    },\n",
    "    evaluation_config={\n",
    "        \"explore\": False,\n",
    "    },\n",
    "    gamma = 1,\n",
    "    target_network_update_freq=500,\n",
    "    buffer_size=100,\n",
    "    adam_epsilon=1e-8,\n",
    "    grad_clip=40,\n",
    "    train_batch_size=32,\n",
    "    framework='torch',\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Register environment\n",
    "ray.shutdown()\n",
    "register_env(env_name, env_config)\n",
    "\n",
    "# Initialize Ray and Build Agent\n",
    "info = ray.init(num_cpus=10, ignore_reinit_error=True, log_to_driver=False)\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n",
    "\n",
    "agent = agents.dqn.DQNTrainer(env=env_name,\n",
    "    config=rl_config)\n",
    "results = []\n",
    "\n",
    "rew = np.nan\n",
    "\n",
    "pbar = tqdm(range(epochs), desc='Training Loop' )\n",
    "\n",
    "for i in pbar:\n",
    "    res = agent.train()\n",
    "    results.append(res)\n",
    "    rew = res['episode_reward_mean']\n",
    "    pbar.set_description(\"reward = %f\" % rew)\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "# Unpack values from each iteration\n",
    "rewards = np.hstack([i['episode_reward_mean'] \n",
    "    for i in results])\n",
    "\n",
    "p = 50\n",
    "\n",
    "mean_rewards = np.array([np.mean(rewards[i-p:i+1]) \n",
    "                if i >= p else np.mean(rewards[:i+1]) \n",
    "                for i, _ in enumerate(rewards)])\n",
    "std_rewards = np.array([np.std(rewards[i-p:i+1])\n",
    "               if i >= p else np.std(rewards[:i+1])\n",
    "               for i, _ in enumerate(rewards)])\n",
    "\n",
    "#plt.figure(constrained_layout=True, figsize=(16, 9))\n",
    "plt.fill_between(np.arange(len(mean_rewards)), \n",
    "                 mean_rewards - std_rewards, \n",
    "                 mean_rewards + std_rewards, \n",
    "                 label='Standard Deviation', alpha=0.3)\n",
    "plt.plot(mean_rewards, label='Mean Rewards')\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.title('Training Rewards')\n",
    "plt.legend()\n",
    "plt.savefig(\"Results_Rewards.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Loss (td error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_err = [\n",
    "    i['info']['learner']['default_policy']['mean_td_error'] \n",
    "    for i in results]\n",
    "\n",
    "\n",
    "p = 50\n",
    "\n",
    "mean_td_err = np.array([np.mean(td_err[i-p:i+1]) \n",
    "                if i >= p else np.mean(td_err[:i+1]) \n",
    "                for i, _ in enumerate(td_err)])\n",
    "std_td_err = np.array([np.std(td_err[i-p:i+1])\n",
    "               if i >= p else np.std(td_err[:i+1])\n",
    "               for i, _ in enumerate(td_err)])\n",
    "\n",
    "#plt.figure(constrained_layout=True, figsize=(16, 9))\n",
    "plt.fill_between(np.arange(len(mean_td_err)), \n",
    "                 mean_td_err - std_td_err, \n",
    "                 mean_td_err + std_td_err, \n",
    "                 label='Standard Deviation', alpha=0.3)\n",
    "plt.plot(mean_td_err, label='Mean td_err')\n",
    "plt.ylabel('td_err')\n",
    "plt.xlabel('Episode')\n",
    "plt.title('Training td_err')\n",
    "plt.legend()\n",
    "plt.savefig(\"Results_TD_err.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through the environment with the agent to get final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_gym.create_env import create_env\n",
    "\n",
    "env = create_env(env_name, env_config=env_config)\n",
    "\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot final output\n",
    "final_df = info['final_state_data']\n",
    "for cls in final_df['action'].unique():\n",
    "    plt.scatter(final_df[final_df['action'] == cls][1], final_df[final_df['action'] == cls][2])\n",
    "plt.savefig(\"rl_predicted_output.svg\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dab403dcfa4a64bee3ff417c650bc5376500f360e3ead239cab01b685475af7b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
